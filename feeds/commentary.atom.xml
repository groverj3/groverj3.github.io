<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jeff Grover. Bioinformatician. - commentary</title><link href="https://groverj3.github.io/" rel="alternate"></link><link href="https://groverj3.github.io/feeds/commentary.atom.xml" rel="self"></link><id>https://groverj3.github.io/</id><updated>2019-08-09T00:00:00-07:00</updated><subtitle>Bioinformatician/Ph.D. Candidate&lt;br&gt;
&lt;a href="https://cals.arizona.edu/research/mosherlab/Mosher_Lab/Home.html" target="_blank"&gt;The Mosher Lab&lt;/a&gt;
@ &lt;a href="https://www.arizona.edu/" target="_blank"&gt;The University of Arizona&lt;/a&gt;</subtitle><entry><title>Suggestions for Reproducible Bioinformatic Analyses</title><link href="https://groverj3.github.io/articles/2019-08-09_suggestions-for-reproducible-bioinformatic-analyses.html" rel="alternate"></link><published>2019-08-09T00:00:00-07:00</published><updated>2019-08-09T00:00:00-07:00</updated><author><name>Jeffrey Grover</name></author><id>tag:groverj3.github.io,2019-08-09:/articles/2019-08-09_suggestions-for-reproducible-bioinformatic-analyses.html</id><summary type="html">&lt;p&gt;Bioinformatic analyses often require lengthy workflows or pipleines, where the
output of program A feeds into program B, and so on. These programs may also not
output their results in a format which is convenient to use in the subsequent
steps, requiring writing a conversion script, or piping its output â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Bioinformatic analyses often require lengthy workflows or pipleines, where the
output of program A feeds into program B, and so on. These programs may also not
output their results in a format which is convenient to use in the subsequent
steps, requiring writing a conversion script, or piping its output through yet
another program. This means that something as simple as running a differential
expression experiment still requires several steps. If you aren't careful this
can result in an incredibly messy filesystem. Worse, you may not remember which
programs or scripts were run on each file, and with which options. This is a huge
issue out there and likely a good reason why it's so hard to reproduce results
even when the same underlying data is used. Additionally, you'll inevitably need
to spend time doing iterative analysis. This also needs to be documented and
reproducible.&lt;/p&gt;
&lt;p&gt;In this post I'll be explaining a few methods that we can use organize this
situation before it drives you or your coworkers mad. Depending, of course, on
the level of automation and reproucibility required of the workflow.&lt;/p&gt;
&lt;h3&gt;Suggestion 1: Interactive Terminal Sessions Are For Development &lt;strong&gt;Only&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;There is most definitely a time and a place for testing things out in your
terminal. When you're learning to use a new program, needing to check the
&lt;code&gt;--help&lt;/code&gt; or &lt;code&gt;man&lt;/code&gt; pages, figuring out how to glue together programs A and B, etc.
However, in-depth analyses for publication should not be done in this manner.&lt;/p&gt;
&lt;p&gt;This is because after running your analysis you may have absolutely no record of
what was run! Of course, some (but, criticall, not all!) programs will export a
log file. However, not all do. You can quickly end up in a situation where you
have no idea which script was run on which file. So, reserve the interactive
terminal sessions for those use cases above.&lt;/p&gt;
&lt;h3&gt;Suggestion 2: Interactive Data Manipulation Should Be Performed in R or Jupyter Notebooks&lt;/h3&gt;
&lt;p&gt;Don't use Excel. I repeat, don't use Excel.&lt;/p&gt;
&lt;p&gt;Ok, Excel has its uses. However, if you're doing complex data analysis
it's very easy to get to the scale that you'll regret using Excel quickly.
Luckily the &lt;em&gt;entire&lt;/em&gt; &lt;a href="R programming language"&gt;https://www.r-project.org/&lt;/a&gt; was 
designed for this, and &lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt; with
&lt;a href="https://pandas.pydata.org/index.html"&gt;Pandas&lt;/a&gt; provides some similar tools. In
addition to scale, you also have no real record of what was done in an Excel
workbook. When you combine R or Python with computational notebooks you can run
code, and see the direct output of that code below it. This tracks everything
that you've run and its outputs.&lt;/p&gt;
&lt;p&gt;Even though I do most of my interactive analysis and figure-making in R, I still
prefer Jupyter Notebooks over R Notebooks. This is because they're more widely
used, and Jupyter is extensible to multiple languages. Installing the
&lt;a href="https://irkernel.github.io/"&gt;R Kernel&lt;/a&gt; is very simple.&lt;/p&gt;
&lt;h3&gt;Suggestion 3: Single-run Pipelines Should be Automated With Shell Scripts&lt;/h3&gt;
&lt;p&gt;When you write a one-off pipeline it should still be automated with a script.
This enables reproducibility. In a perfect world you'd list the version of each
piece of software in the pipeline as well. This could result in a single shell
script file, or separate ones for each step. You may not know the next step
a-priori. These shell scripts should clearly indicate the date of creation and
the script's purpose. This is a simple example for one step in a single-use
pipeline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env bash&lt;/span&gt;

&lt;span class="c1"&gt;# Author: Jeffrey Grover&lt;/span&gt;
&lt;span class="c1"&gt;# Date: 2019-07-24&lt;/span&gt;
&lt;span class="c1"&gt;# Purpose: Extract reads over small RNA loci groups with bedtools intersect&lt;/span&gt;

&lt;span class="c1"&gt;# Use bedtools intersect and pipe to bam2fq&lt;/span&gt;

&lt;span class="nv"&gt;align_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;~/large_data/2019-06-28_aligned_reads&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; bed_file in ./srna_groups/*.bed&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;

    &lt;span class="nv"&gt;bed_filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;basename &lt;span class="nv"&gt;$bed_file&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;out_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;bed_filename&lt;/span&gt;&lt;span class="p"&gt;%.bed&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;_reads
    mkdir &lt;span class="s2"&gt;&amp;quot;./&lt;/span&gt;&lt;span class="nv"&gt;$out_dir&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; bamfile in &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;align_dir&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/*.bam&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;

        &lt;span class="nv"&gt;bamfile_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;basename &lt;span class="nv"&gt;$bamfile&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;

        bedtools intersect &lt;span class="se"&gt;\&lt;/span&gt;
                -ubam &lt;span class="se"&gt;\&lt;/span&gt;
                -a &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$bamfile&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
                -b &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$srna_file&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
            &lt;span class="p"&gt;|&lt;/span&gt; samtools bam2fq -n - &amp;gt; &lt;span class="s2"&gt;&amp;quot;./&lt;/span&gt;&lt;span class="nv"&gt;$out_dir&lt;/span&gt;&lt;span class="s2"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;$bamfile_name&lt;/span&gt;&lt;span class="s2"&gt;.fq&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;done&lt;/span&gt;

    pigz -p &lt;span class="m"&gt;10&lt;/span&gt; ./&lt;span class="nv"&gt;$out_dir&lt;/span&gt;/*.fq
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I work with a lot of small RNA sequencing, and I recently needed to extract reads
from several different groups of small RNA loci I'd defined. It's relatively
simple to use &lt;code&gt;bedtools intersect&lt;/code&gt; with your interesting loci as a .bed file and
pipe that output to &lt;code&gt;samtools bam2fq&lt;/code&gt;. This isn't the kind of thing that's a
standard analysis I need to do and it's not very long. Therefore, to enable it to
be reproducible writing a quick shell script like this is the way to go. The
comment lines also carry enough information to tell someone what it does.&lt;/p&gt;
&lt;h3&gt;Suggestion 4: Long Pipelines Should Have a &lt;strong&gt;W i d e&lt;/strong&gt; Directory Structure&lt;/h3&gt;
&lt;p&gt;What does this mean? It means this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;groverj3@x1-carbon wgbs_snakemake&lt;span class="o"&gt;]&lt;/span&gt;$ ls
1_fastqc_raw                4_methyldackel_mbias    config.yaml  README.md         Snakefile
2_trim_galore               5_methyldackel_extract  input_data   reference_genome  temp_data
3_aligned_sorted_markdupes  6_mosdepth              LICENSE      scripts
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and not this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wgbs_snakemake/1_fastqc_raw/2_trim_galore/3_aligned_sorted_markdupes/4_methyldackel_mbias/5_methyldackel_extract/6_mosdepth
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This makes navigating your directory structure much less of a pain. Especially
when a pipeline is several steps long.&lt;/p&gt;
&lt;h3&gt;Suggestion 5: Automate Often-run Pipelines With Workflow Managers&lt;/h3&gt;
&lt;p&gt;If there is a particular pipeline that you run frequently then consider using a
workflow manager. Options include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://snakemake.readthedocs.io/en/stable/"&gt;snakemake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nextflow.io/"&gt;Nextflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scipipe.org/"&gt;scipipe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pharmbio/sciluigi"&gt;SciLuigi&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My vote goes to Snakemake with Nextflow as a close second. These tools require
some fiddling to transfer over an existing pipeline to fit their framework, but
what you gain is reproducibility and automation. Additionally, they all utilize
threading with parallel steps better than your BASH script does. They also work
with HPC job submission frameworks (SLURM, PBS, etc.) and containers.&lt;/p&gt;
&lt;p&gt;Writing these workflows is beyond the scope of this article, but definitely worth
writing in detail about in a future one!&lt;/p&gt;
&lt;p&gt;A word of caution: it's easy to think, "Oh, I'm only going to analyze bisulfite
sequencing this one time" only to find yourself running your workflow several
times as you acquire more data. There are also some freely available workflows
already written that you can check out!&lt;/p&gt;
&lt;p&gt;(Shameless plug for &lt;a href="https://github.com/groverj3/wgbs_snakemake"&gt;mine&lt;/a&gt;)&lt;/p&gt;
&lt;h3&gt;Suggestion 6: Containerize!&lt;/h3&gt;
&lt;p&gt;Wrap-up your workflow and its required software in a container for the ultimate
write once run anywhere solution. You can make a 
&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; container with your entire workflow which can
then be used on your server, or cloud computing. However, in order to run this in
an HPC environment you'll need to run it through 
&lt;a href="https://sylabs.io/"&gt;Singularity&lt;/a&gt; instead. That's fine though! Singularity can
run Docker containers, and you'll already have one to use for cloud compute if
needed.&lt;/p&gt;
&lt;h3&gt;Wrapping up&lt;/h3&gt;
&lt;p&gt;Hopefully you've found this informative and helpful. Next time I'll be back with
more practical examples.&lt;/p&gt;</content><category term="bioinformatics"></category><category term="thoughts"></category><category term="workflows"></category></entry><entry><title>Variations on RNAseq Workflows for DEG Analysis</title><link href="https://groverj3.github.io/articles/2019-07-09_variations-on-rnaseq-workflows-for-deg-analysis.html" rel="alternate"></link><published>2019-07-09T00:00:00-07:00</published><updated>2019-07-09T00:00:00-07:00</updated><author><name>Jeffrey Grover</name></author><id>tag:groverj3.github.io,2019-07-09:/articles/2019-07-09_variations-on-rnaseq-workflows-for-deg-analysis.html</id><summary type="html">&lt;p&gt;When analyzing RNAseq you're faced with many possible analysis pipelines. The
biggest decision you need to make is what the purpose of your experiment is. I
will make the assumption that &lt;em&gt;most&lt;/em&gt; of the time people want to determine which
genes are differentially expressed between two samples, genotypes, conditions,
etc â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;When analyzing RNAseq you're faced with many possible analysis pipelines. The
biggest decision you need to make is what the purpose of your experiment is. I
will make the assumption that &lt;em&gt;most&lt;/em&gt; of the time people want to determine which
genes are differentially expressed between two samples, genotypes, conditions,
etc. In DEG analyss you are interested in gene-level expression. This means you
are &lt;strong&gt;not&lt;/strong&gt; interested in differential isoforms/transcripts or alternative
splicing.The absolute most simple version of this is simply having control and
experimental samples (preferably with &amp;gt;= 3 biological replicates each). However,
this isn't as straightforward as firing up your favorite aligner and going to
town on the data. There are other considerations.&lt;/p&gt;
&lt;h3&gt;I Have a High Quality Annotated Reference Genome or Transcriptome&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;My Reference Genome is High Quality&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Align reads to reference genome (STAR, HISAT2)&lt;/li&gt;
&lt;li&gt;Count reads per gene (HTSeq-count, summarizeOverlaps, featurecounts)&lt;/li&gt;
&lt;li&gt;DEG Analysis (DESeq2, edgeR)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is the standard workflow that you're probably accustomed to. Note: it is
very important to use a &lt;em&gt;modern&lt;/em&gt; splicing-aware aligner. Do not use bowtie. Both
STAR and HISAT2 are very fast compared to older aligners and are designed for
RNAseq. Their default options are generally appropriate for most simple
experimental designs. As a bonus, STAR can actually do step 2 itself, although
the output format is kind of clunky.&lt;/p&gt;
&lt;p&gt;This workflow is a good general purpose one in model organisms, and nobody will
fault you for using it there. However, there are potentially better options.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;My Annotation/transcriptome is High Quality&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pseudoalignment-based abundance estimation (Salmon, Kallisto)&lt;/li&gt;
&lt;li&gt;Aggregate abundances per gene from transcripts (tximport)&lt;/li&gt;
&lt;li&gt;DEG Analysis (DESeq2, edgeR)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This workflow may actually be better
(&lt;a href="https://f1000research.com/articles/4-1521/v2"&gt;ref&lt;/a&gt;) even if you have a
reference genome. I've always assumed that reference-genome alignment is superior
when you have a good reference, but apparently this is not necessarily the case
for the reasons detailed here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt; very fast, potentially more accurate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt; no .bam file is generated so you can't look at positional information
from your reads, no ability to discover new transcripts later from your
alignments.&lt;/p&gt;
&lt;p&gt;Either of these workflows will work fine in this situation, and the better your
genome is the closer the first will likely approximate the second. Though, I now
believe that the second workflow should be the standard if your goal is purely
DEG analysis. There are still a lot of good reasons to want a .bam file, though
nothing is stopping you from aligning your reads anyway for future-use.&lt;/p&gt;
&lt;h3&gt;My Genome/Transcriptome is Incomplete&lt;/h3&gt;
&lt;p&gt;In this case you have some deicsions to make, yet again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Genome is Good but Annotations Are Poor&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Align to reference genome (STAR, HISAT2)&lt;/li&gt;
&lt;li&gt;Assemble transcripts, genome-guided (Stringtie)&lt;/li&gt;
&lt;li&gt;Aggregate abundances per gene from transcripts (tximport)&lt;/li&gt;
&lt;li&gt;DEG Analysis (DESeq2, edgeR)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another option here is to use a tool like
&lt;a href="https://github.com/PASApipeline/PASApipeline/wik"&gt;PASA&lt;/a&gt; to update the
existing annotations if they exist. I've run that pipeline. It's very quirky, a
pain to get running, and if you don't need genomic coordinates I'd avoid it. You
could also use Salmon/Kallisto with StringTie's transcripts, without using its
quantification, but this seems to be an unnecessary step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Genome and Transcriptome Are Poor&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Assemble transcriptome (Trinity)&lt;/li&gt;
&lt;li&gt;Pseudoalignment-based abundance estimation (Salmon, Kallisto)&lt;/li&gt;
&lt;li&gt;Aggregate abundances per gene from transcripts (tximport)&lt;/li&gt;
&lt;li&gt;DEG Analysis (DESeq2, edgeR)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case you're going to want to do a thorough &lt;em&gt;de-novo&lt;/em&gt; transcriptome
assembly using something like
&lt;a href="https://github.com/trinityrnaseq/trinityrnaseq/wiki"&gt;Trinity&lt;/a&gt;. This
transcriptome can then be used for pseudoalignment-based abundance estimation and
then DEGs can be determined after aggregation of isoform abundances. Trinity can
be quite a resource hog, so you're going to want to
&lt;a href="https://downloadmoreram.com/"&gt;get more ram&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Why Not Cufflinks/Stringtie For Transcript Assembly In Model Organisms?&lt;/h3&gt;
&lt;p&gt;First of all, don't use Cufflinks. Stringtie is essentially a more modern
Cufflinks that's
&lt;a href="https://ccb.jhu.edu/software/stringtie/index.shtml?t=faq#comp"&gt;faster and more accurate&lt;/a&gt;.
Secondly, if you're working in a well annotated genome chances are that "novel
transcripts" you find are more likely noise, or not biologically meaningful
(unless you know better for your use-case!).&lt;/p&gt;
&lt;h3&gt;Concluding Thoughts&lt;/h3&gt;
&lt;p&gt;The paper detailing that transcript abundances, when aggregated to gene level,
improve DEG analysis is particularly interesting. This makes me rethink my usual
assumption and I now believe that tools like Salmon or Kallisto should be the go
to tools for DEG analysis when you have a good transcriptome to work with.&lt;/p&gt;
&lt;p&gt;However, I still think it's worthwhile to align your reads and generate a .bam
file. There are many types of visualizations and comparisons that you simply
can't do without them. For example, calculating coverage over featutres of
interest. If you must compare expression of genes across multiple samples or from
different experiments then you'll probably want to convert your expression values
to some normalized measurement. In this case you can use FPKM or TPM, though the
consensus seems to be that TPM is the way to go these days.&lt;/p&gt;
&lt;p&gt;And, at the end of the day you know that an out-of-date collaborator is probably
going to ask you for FPKM measurements or something anyway.&lt;/p&gt;</content><category term="bioinformatics"></category><category term="thoughts"></category><category term="rnaseq"></category><category term="workflows"></category></entry></feed>